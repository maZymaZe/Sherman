#include "Timer.h"
#include "Tree.h"
#include "zipf.h"

#include <city.h>
#include <stdlib.h>
#include <time.h>
#include <unistd.h>
#include <thread>
#include <vector>
// #include <mutex>
#include <fstream>
#include <iostream>
#include <string>
// std::mutex mutex;

#define USE_CORO
#define TEST_EPOCH 10

extern uint64_t cache_miss[MAX_APP_THREAD];
extern uint64_t cache_hit[MAX_APP_THREAD];
extern uint64_t lock_fail[MAX_APP_THREAD];
extern uint64_t try_lock[MAX_APP_THREAD];
extern uint64_t try_write_op[MAX_APP_THREAD];
extern uint64_t read_retry[MAX_APP_THREAD];
extern uint64_t try_read[MAX_APP_THREAD];

int kReadRatio;
int kThreadCount;
int kNodeCount;

uint64_t kKeySpace = 6 * define::MB;
double kWarmRatio = 1;
double zipfan = 0.99;
int kCoroCnt = 2;

std::thread th[MAX_APP_THREAD];
uint64_t tp[MAX_APP_THREAD][MAX_CORO_NUM];

extern volatile bool need_stop;
extern uint64_t latency[MAX_APP_THREAD][MAX_CORO_NUM][LATENCY_WINDOWS];
uint64_t latency_th_all[LATENCY_WINDOWS];

Tree* tree;
DSM* dsm;

inline Key to_key(uint64_t k) {
    return int2key((CityHash64((char*)&k, sizeof(k)) + 1) % kKeySpace);
}

class RequsetGenBench : public RequstGen {
   public:
    RequsetGenBench(DSM* dsm, int coro_id)
        : id(dsm->getMyThreadID()), coro_id(coro_id) {
        seed = rdtsc();
        mehcached_zipf_init(&state, kKeySpace, zipfan,
                            (rdtsc() & (0x0000ffffffffffffull)) ^ id);
    }

    Request next() override {
        Request r;
        uint64_t dis = mehcached_zipf_next(&state);

        r.k = to_key(dis);
        r.v = ++val;
        r.is_search = rand_r(&seed) % 100 < kReadRatio;

        tp[id][coro_id]++;
        return r;
    }

   private:
    int id;
    int coro_id;
    uint64_t val = 0;
    unsigned int seed;
    struct zipf_gen_state state;
};

RequstGen* gen_func(DSM* dsm,
                    Request* req,
                    int req_num,
                    int coro_id,
                    int coro_cnt) {
    return new RequsetGenBench(dsm, coro_id);
}

void work_func(Tree* tree, const Request& r, CoroContext* ctx, int coro_id) {
    if (r.is_search) {
        Value v;
        tree->search(r.k, v, ctx, coro_id);
    } else {
        tree->insert(r.k, r.v, ctx, coro_id);
    }
}

Timer bench_timer;
std::atomic<int64_t> warmup_cnt{0};
std::atomic_bool ready{false};

void thread_load(int id) {
    // use 8 threads to load ycsb
    uint64_t all_loader_thread =
        std::min(kThreadCount, 8) * dsm->getClusterSize();
    uint64_t loader_id = std::min(kThreadCount, 8) * dsm->getMyNodeID() + id;
    printf("I am loader %lu\n", loader_id);

    uint64_t end_warm_key = kWarmRatio * kKeySpace;
    for (uint64_t i = 1; i < end_warm_key; ++i) {
        if (i % all_loader_thread == loader_id) {
            tree->insert(to_key(i), i * 2);
        }
    }
    printf("loader %lu load finish\n", loader_id);
    // uint64_t end_warm_key = kWarmRatio * kKeySpace;
    // for (uint64_t i = 1; i < end_warm_key; ++i) {
    //   if (i % all_thread == my_id) {
    //     mutex.lock();  // safely insert
    //     tree->insert(to_key(i), i * 2);
    //     mutex.unlock();
    //   }
    // }
}

void thread_run(int id) {
    bindCore(id * 2 + 1);

    dsm->registerThread();

    uint64_t all_thread = kThreadCount * dsm->getClusterSize();
    uint64_t my_id = kThreadCount * dsm->getMyNodeID() + id;

    printf("I am %lu\n", my_id);

    if (id == 0) {
        bench_timer.begin();
    }

    if (id < std::min(kThreadCount, 8)) {
        thread_load(id);
    }

    warmup_cnt.fetch_add(1);

    if (id == 0) {
        while (warmup_cnt.load() != kThreadCount)
            ;
        printf("node %d finish\n", dsm->getMyNodeID());
        dsm->barrier("warm_finish");

        uint64_t ns = bench_timer.end();
        printf("warmup time %lds\n", ns / 1000 / 1000 / 1000);

        ready = true;
        warmup_cnt.store(0);
    }

    while (warmup_cnt.load() != 0)
        ;

#ifdef USE_CORO
    tree->run_coroutine(gen_func, work_func, kCoroCnt);
#else
    /// without coro
    Timer timer;
    auto gen = new RequsetGenBench(dsm, 0);
    auto thread_id = dsm->getMyThreadID();

    while (!need_stop) {
        auto r = gen->next();

        timer.begin();
        work_func(tree, r, nullptr, 0);
        auto us_10 = timer.end() / 100;

        if (us_10 >= LATENCY_WINDOWS) {
            us_10 = LATENCY_WINDOWS - 1;
        }
        latency[thread_id][0][us_10]++;
    }
#endif
    printf("thread %d exit.\n", id);
}

void parse_args(int argc, char* argv[]) {
    if (argc != 6) {
        printf(
            "Usage: ./zipfian_test kNodeCount kReadRatio kThreadCount zipfan "
            "kCoroCnt\n");
        exit(-1);
    }

    kNodeCount = atoi(argv[1]);
    kReadRatio = atoi(argv[2]);
    kThreadCount = atoi(argv[3]);
    zipfan = atof(argv[4]);
    kCoroCnt = atoi(argv[5]);

    printf(
        "kNodeCount %d, kReadRatio %d, kThreadCount %d, zipfan %lf, kCoroCnt "
        "%d\n",
        kNodeCount, kReadRatio, kThreadCount, zipfan, kCoroCnt);
}

void save_latency(int epoch_id) {
    // sum up local latency cnt
    for (int i = 0; i < LATENCY_WINDOWS; ++i) {
        latency_th_all[i] = 0;
        for (int k = 0; k < MAX_APP_THREAD; ++k)
            for (int j = 0; j < MAX_CORO_NUM; ++j) {
                latency_th_all[i] += latency[k][j][i];
            }
    }
    // store in file
    std::ofstream f_out("../us_lat/epoch_" + std::to_string(epoch_id) + ".lat");
    f_out << std::setiosflags(std::ios::fixed) << std::setprecision(1);
    if (f_out.is_open()) {
        for (int i = 0; i < LATENCY_WINDOWS; ++i) {
            f_out << i / 10.0 << "\t" << latency_th_all[i] << std::endl;
        }
        f_out.close();
    } else {
        printf("Fail to write file!\n");
        assert(false);
    }
    memset(latency, 0,
           sizeof(uint64_t) * MAX_APP_THREAD * MAX_CORO_NUM * LATENCY_WINDOWS);
}

// void cal_latency() {
//   uint64_t all_lat = 0;
//   for (int i = 0; i < LATENCY_WINDOWS; ++i) {
//     latency_th_all[i] = 0;
//     for (int k = 0; k < MAX_APP_THREAD; ++k)
//       for (int j = 0; j < MAX_CORO_NUM; ++j) {
//         latency_th_all[i] += latency[k][j][i];
//     }
//     all_lat += latency_th_all[i];
//   }
//   memset(latency, 0, sizeof(uint64_t) * MAX_APP_THREAD * MAX_CORO_NUM *
//   LATENCY_WINDOWS);

//   uint64_t th50 = all_lat / 2;
//   uint64_t th90 = all_lat * 9 / 10;
//   uint64_t th95 = all_lat * 95 / 100;
//   uint64_t th99 = all_lat * 99 / 100;
//   uint64_t th999 = all_lat * 999 / 1000;

//   uint64_t cum = 0;
//   for (int i = 0; i < LATENCY_WINDOWS; ++i) {
//     cum += latency_th_all[i];

//     if (cum >= th50) {
//       printf("p50 %f\t", i / 10.0);
//       th50 = -1;
//     }
//     if (cum >= th90) {
//       printf("p90 %f\t", i / 10.0);
//       th90 = -1;
//     }
//     if (cum >= th95) {
//       printf("p95 %f\t", i / 10.0);
//       th95 = -1;
//     }
//     if (cum >= th99) {
//       printf("p99 %f\t", i / 10.0);
//       th99 = -1;
//     }
//     if (cum >= th999) {
//       printf("p999 %f\n", i / 10.0);
//       th999 = -1;
//       return;
//     }
//   }
// }

int main(int argc, char* argv[]) {
    parse_args(argc, argv);
    // mehcached_test_zipf(zipfan);

    DSMConfig config;
    config.machineNR = kNodeCount;
    config.threadNR = kThreadCount;
    dsm = DSM::getInstance(config);
    dsm->registerThread();
    tree = new Tree(dsm);

    dsm->barrier("benchmark");

    for (int i = 0; i < kThreadCount; i++) {
        th[i] = std::thread(thread_run, i);
    }

    while (!ready.load())
        ;

    timespec s, e;
    uint64_t pre_tp = 0;
    int count = 0;

    clock_gettime(CLOCK_REALTIME, &s);

    while (!need_stop) {
        sleep(1);
        clock_gettime(CLOCK_REALTIME, &e);
        int microseconds = (e.tv_sec - s.tv_sec) * 1000000 +
                           (double)(e.tv_nsec - s.tv_nsec) / 1000;

        uint64_t all_tp = 0;
        for (int i = 0; i < MAX_APP_THREAD; ++i) {
            for (int j = 0; j < MAX_CORO_NUM; ++j)
                all_tp += tp[i][j];
        }
        uint64_t cap = all_tp - pre_tp;
        pre_tp = all_tp;

        uint64_t all = 0, hit = 0;
        for (int i = 0; i < MAX_APP_THREAD; ++i) {
            all += (cache_hit[i] + cache_miss[i]);
            hit += cache_hit[i];
        }

        uint64_t try_lock_cnt = 0, lock_fail_cnt = 0, try_write_op_cnt = 0;
        for (int i = 0; i < MAX_APP_THREAD; ++i) {
            lock_fail_cnt += lock_fail[i];
            try_lock_cnt += try_lock[i];
            try_write_op_cnt += try_write_op[i];
        }

        uint64_t try_read_cnt = 0, read_retry_cnt = 0;
        for (int i = 0; i < MAX_APP_THREAD; ++i) {
            try_read_cnt += try_read[i];
            read_retry_cnt += read_retry[i];
        }

        clock_gettime(CLOCK_REALTIME, &s);

        // if (++count % 3 == 0) {
        //   cal_latency();
        // }
        tree->clear_debug_info();

        save_latency(++count);
        if (count >= TEST_EPOCH) {
            need_stop = true;
        }

        double per_node_tp = cap * 1.0 / microseconds;
        uint64_t cluster_tp = dsm->sum((uint64_t)(per_node_tp * 1000));

        printf("%d, throughput %.4f\n", dsm->getMyNodeID(), per_node_tp);

        if (dsm->getMyNodeID() == 0) {
            printf("epoch %d passed!\n", count);
            printf("cluster throughput %.3f\n", cluster_tp / 1000.0);
            printf("cache hit rate: %lf\n", hit * 1.0 / all);
            printf("avg. lock/cas fail cnt: %lf\n",
                   lock_fail_cnt * 1.0 / try_lock_cnt);
            printf("read retry rate: %lf\n",
                   read_retry_cnt * 1.0 / try_read_cnt);
            printf("\n");
        }
    }
    printf("[END]\n");
    for (int i = 0; i < kThreadCount; i++) {
        th[i].join();
        printf("Thread %d joined.\n", i);
    }
    tree->statistics();
    dsm->barrier("fin");

    return 0;
}
